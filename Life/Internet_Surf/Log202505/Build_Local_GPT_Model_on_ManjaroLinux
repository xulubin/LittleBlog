# 从零开始：搭建本地知识库 + 大模型

**前提条件**
硬件：机械革命 G16，GTX 4060（8GB 显存），双硬盘，内存假设 16GB 或以上。
系统：Manjaro 已安装（硬盘 1），可以启动并进入桌面环境。
目标：在本地运行 DeepSeek 8B 模型，结合个人文档（如 TXT 文件）构建知识库。
当前状态：假设你刚装好 Manjaro，还没做任何额外配置。

## 一、准备系统环境

首先，我们需要更新系统并安装一些基础工具，就像给房子打地基一样，确保后续工作顺利。

### 1.1 更新系统

打开终端（在 Manjaro 中，通常是点击“终端”图标或按 Ctrl + Alt + T）。
输入以下命令并按回车：

```bash
sudo pacman -Syu
```

**含义**：
`sudo`：以管理员权限运行命令（需要输入你的密码）。
`pacman`：Manjaro 的软件包管理器，像一个“应用商店”。
`-Syu`：同步（S）软件源，刷新（y）数据库，升级（u）所有软件。
**作用**：确保系统和软件是最新版本，避免兼容性问题。
**预期输出**：终端会显示下载和更新的过程，可能需要几分钟，完成后会返回提示符。

### 1.2 安装基础工具

我们需要 Python（编程语言）、Git（下载代码工具）和显卡支持工具（CUDA）。运行：

```bash
sudo pacman -S python python-pip git cuda cudnn
```

**含义**：
`python`：运行模型和知识库脚本需要的语言。
`python-pip`：Python 的包管理器，用来安装额外的库。
`git`：用来从网上下载代码（如 llama.cpp）。
`cuda` 和 `cudnn`：NVIDIA 的工具，让你的 GTX 4060 能加速模型运行。
**作用**：安装这些工具就像准备好“锤子和钉子”，为后续搭建做准备。
**验证**：
检查 Python：`python --version`（应显示类似 Python 3.11.x）。
检查 Git：`git --version`。
检查显卡：`nvidia-smi`（应显示 GTX 4060 信息）。

## 二、创建虚拟环境


虚拟环境就像一个独立的“工作间”，避免不同工具互相干扰。

### 2.1 创建虚拟环境

在终端输入：

```bash
python -m venv ~/knowledge_env
```

**含义**：
`python -m venv`：调用 Python 的虚拟环境模块。
`~/knowledge_env`：创建一个文件夹（在你的家目录下），用来存放虚拟环境。
**作用**：相当于建了一个干净的小房间，只装我们需要的工具。
**结果**：你会在 ~/（即 /home/你的用户名/）下看到 knowledge_env 文件夹。

### 2.2 激活虚拟环境

输入以下命令激活：

```bash
source ~/knowledge_env/bin/activate
```

**含义**：
`source`：执行一个脚本。
`~/knowledge_env/bin/activate`：激活虚拟环境的脚本。
**作用**：进入这个“工作间”，后续安装的工具只影响这里。
**结果**：终端提示符前会出现 `(knowledge_env)`，表示成功激活。

### 2.3 验证虚拟环境

检查当前环境：

```bash
pip list
```

**含义**：列出当前环境安装的 Python 包。
**结果**：应该只看到 `pip` 和 `setuptools`，说明环境是干净的。

> **Tips**：每次打开新终端时，都需要重新运行 `source ~/knowledge_env/bin/activate` 来进入环境。

### 2.4 其他说明：手动退出虚拟环境

直接在终端输入：

```Bash
deactivate
```

- **效果**：退出当前虚拟环境，终端提示符前的 `(test_env)` 会消失。
- **验证**：执行 `which python` 或 `pip --version`，路径应恢复为系统默认路径。

## 三、安装 Python 依赖

在虚拟环境中安装搭建知识库需要的“零件”。

### 3.1 安装必要的库

在激活虚拟环境后（终端有 `(knowledge_env)`），运行：

```bash
pip install langchain chromadb sentence-transformers llama-cpp-python
```

**含义**：
`pip install`：Python 的“商店”命令，下载并安装软件包。
`langchain`：连接模型和知识库的桥梁。
`chromadb`：存储文档的数据库，像一个“智能文件夹”。
`sentence-transformers`：把文档变成数字（向量），方便模型理解。
`llama-cpp-python`：运行 DeepSeek 模型的工具。
**作用**：这些库是搭建知识库的核心组件。
**预期输出**：终端会显示下载和安装进度，可能需要几分钟。

### 3.2 加速安装

#### **方法一：使用国内镜像源**

直接通过 `-i` 参数指定国内镜像源，加速下载：

```Bash
pip install langchain chromadb sentence-transformers llama-cpp-python \
  -i https://pypi.tuna.tsinghua.edu.cn/simple \
  --trusted-host pypi.tuna.tsinghua.edu.cn
```

**推荐镜像源**（可替换）：

- 清华源：`https://pypi.tuna.tsinghua.edu.cn/simple`
- 阿里云：`https://mirrors.aliyun.com/pypi/simple`
- 豆瓣：`http://pypi.douban.com/simple`

#### **方法二：预下载大文件（针对 `llama-cpp-python`）**

`llama-cpp-python` 需要编译 C++ 代码，可手动下载预编译的二进制文件：

1. 访问 [llama-cpp-python Releases](https://github.com/abetlen/llama-cpp-python/releases)
2. 根据系统环境下载对应的 `.whl` 文件（如 `llama_cpp_python-0.2.26-cp310-cp310-win_amd64.whl`）
3. 本地安装：

```Bash
pip install 你的下载路径/llama_cpp_python-*.whl
```

#### 方法三：设置超时和重试

网络不稳定时，增加超时时间和重试次数：

```Bash
pip install langchain chromadb sentence-transformers llama-cpp-python \
  --default-timeout=1000 \
  --retries=5 \
  -i https://pypi.tuna.tsinghua.edu.cn/simple
```

#### 方法四：单独安装`llama-cpp-python`

**安装必要依赖（AUR加速）**

```Bash
# 1. 先安装yay（AUR助手）
sudo pacman -S yay

# 2. 配置AUR镜像源（加速下载）
echo 'Server = https://aur.tuna.tsinghua.edu.cn/$repo/$arch' | sudo tee -a /etc/pacman.conf

# 3. 安装关键依赖
yay -S openblas cuda --noconfirm  # 自动确认所有提示
```

**配置编译参数**

```Bash
# 设置清华镜像源（解决下载慢）
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

# 设置CUDA编译参数（关键！）
export CMAKE_ARGS="-DLLAMA_CUDA=on -DCMAKE_CUDA_COMPILER=/opt/cuda/bin/nvcc"
```

**安装llama-cpp-python**

```Bash
# 使用prime-run调用独立显卡
prime-run pip install llama-cpp-python --force-reinstall --verbose
```

**参数说明**：

- `--force-reinstall`：确保覆盖之前的错误安装
- `--verbose`：显示详细进度便于排查问题



---

## 四、安装和配置 llama.cpp

`llama.cpp` 是运行 .gguf 模型的引擎，我们需要下载并编译它。

### 4.1 下载 llama.cpp

在终端输入：

```bash
git clone https://github.com/ggerganov/llama.cpp.git

#使用镜像（可选）
# 替换域名（选择以下任一镜像站）
git clone https://github.com.cnpmjs.org/ggerganov/llama.cpp.git
# 或
git clone https://gitclone.com/github.com/ggerganov/llama.cpp.git
# 或
git clone https://hub.fgit.ml/ggerganov/llama.cpp.git
```

**含义**：
`git clone`：从 GitHub 下载代码。
`https://github.com/ggerganov/llama.cpp.git`：代码的地址。
**作用**：把工具的“蓝图”下载到本地。
**结果**：当前目录下出现 `llama.cpp` 文件夹。

### 4.2 进入目录并编译

a.**进入文件夹**：

```bash
cd llama.cpp
```

**含义**：cd 是“change directory”，切换到 llama.cpp 文件夹。
~~b.**编译代码**：~~

```bash
~~make~~
```

~~**含义**：make 是把代码“组装”成可执行程序的命令。~~
~~**作用**：生成运行模型的工具。~~
~~**结果**：编译完成后，文件夹里会有 `main` 等可执行文件。~~

>  **Tips**：如果编译出错，可能是缺少编译工具，运行 `sudo pacman -S base-devel` 安装后再试。

**#2025.03.04更新#**
The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md。

b.**CMake编译**

```bash
# 创建构建目录
mkdir -p build
cd build

# CMake配置（启用CUDA支持）
cmake .. -DLLAMA_CUDA=ON \
         -DCMAKE_CUDA_COMPILER=/opt/cuda/bin/nvcc \
         -DBUILD_SHARED_LIBS=ON

# 编译项目（使用所有CPU核心）
make -j$(nproc)

```

c.**验证构建结果**

`./bin/llama-cli --help | grep -i cuda`   /指令更新_2025.03.04

```bash
# 应显示CUDA相关选项
（删）./bin/main --help | grep "CUDA" 

#更新
./bin/llama-cli --help | grep -i cuda
```

> **[2025.03.04]核心可执行文件已重命名**

```bash
# 替代原 main 的功能
./bin/llama-cli       # 命令行交互主程序
./bin/llama-server    # API服务器
./bin/llama-bench     # 性能测试工具
./bin/llama-quantize  # 模型量化工具


# 使用 llama-cli 检查CUDA支持
./bin/llama-cli --help | grep -i cuda

# 预期输出应包含CUDA相关参数：
  --n-gpu-layers    Number of layers to store in VRAM (default: 0, -1 for all) [CUDA]
  --main-gpu        Main GPU to use (default: 0) [CUDA]
```



```markdown
>> 一些补充说明

（Knowledge_env）   ~/llama.cpp/build    master  ./bin/llama-cli --help | grep -i cuda
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes



1. 验证结果解读

Bash
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes

    ✅ ggml_cuda_init 表示CUDA初始化成功
    ✅ 检测到你的 NVIDIA RTX 4060 笔记本GPU（计算能力8.9）
    ✅ 环境中的CUDA驱动和运行时库已正确配置

2. 实际启用CUDA加速

在运行模型时，必须通过参数显式指定使用GPU：

Bash
# 示例：使用20层模型参数加载到GPU显存
./bin/llama-cli -m /path/to/model.gguf \
  --n-gpu-layers 20 \
  -p "AI will change the world"

关键参数：

    --n-gpu-layers 20：将前20层模型加载到GPU（值越大显存占用越高）
    -ngl -1：强制所有层使用GPU（需足够显存）

3. 性能对比测试

Bash
# 纯CPU模式（禁用GPU）
./bin/llama-cli -m model.gguf --n-gpu-layers 0 -p "test"

# GPU加速模式
./bin/llama-cli -m model.gguf --n-gpu-layers 20 -p "test"

观察输出中的 eval time 差异，GPU模式应有显著速度提升。
4. 高级监控技巧
实时查看GPU利用率

Bash
watch -n 0.5 nvidia-smi  # 每0.5秒刷新GPU状态

运行推理时应看到 Volatile GPU-Util 数值上升。
日志级调试

Bash
GGML_CUDA_DEBUG=1 ./bin/llama-cli -m model.gguf --n-gpu-layers 20 2>&1 | grep cuda

输出包含 cuda buffer 等关键词即表示GPU内存被实际使用。
5. 性能优化建议
参数	作用	推荐值
--tensor-split	多GPU显存分配	如 --tensor-split 0:8GB
--ctx-size	上下文窗口大小	根据显存调整（如4096）
--batch-size	并行处理量	768-2048
6. 常见问题解决
显存不足错误

Bash
CUDA error: out of memory

解决方案：

Bash
减小 --n-gpu-layers 数值
或使用量化模型（如q4_k_m.gguf）

计算能力不匹配

Bash
CUDA error: no kernel image is available for execution

需重新编译指定计算能力：

Bash
cmake .. -DLLAMA_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=89  # 计算能力8.9对应sm89

结论

你的环境已完全支持CUDA加速，现在可以享受GPU带来的性能提升了！后续建议尝试不同模型和量化版本以找到最佳平衡点。


```



## 五、下载 DeepSeek 模型

我们需要从 ModelScope 下载模型文件。

### 5.1 下载模型

打开浏览器，访问 ModelScope。
搜索 `DeepSeek-R1-Distill-Llama-8B-Q4_K_M`。
点击下载 .gguf 文件（可能需要注册账号）。
下载完成后，将文件移动到 `~/models/`：

- 创建文件夹：

```bash
mkdir ~/models
```

- 假设下载的文件在 ~/Downloads，移动它：

```bash
mv ~/Downloads/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf ~/models/deepseek.gguf
```

**含义**：mv 是“move”，把文件移到指定位置。

### 5.2 测试模型（可选）

在 ~~llama.cpp~~ (`konwledge_env`环境中，`~/llama.cpp/build`路径下 )目录下测试：

```bash
#知识点复习，`source ~/knowledge_env/bin/activate`激活环境
			#`cd llama.cpp/build`切换路径

./bin/llama-cli -m ~/models/deepseek.gguf --prompt "你好，这是什么模型？"
```

**含义**：
`./main`：运行编译好的程序。
`-m`：指定模型路径。
`--prompt`：给模型一个问题。

**结果**：如果成功，终端会输出模型的回答。



## 六、创建知识库

### 6.1 准备文档目录

是否需要在虚拟环境中执行？否
这是文件系统操作，与 Python 无关。

#### 1.创建文档目录（如果尚未创建）：

```bash
mkdir -p ~/Documents/knowledge_base
```

* **命令含义**：

  - `mkdir`：创建目录。

  - `-p`：如果父目录（如 ~/Documents）不存在，自动创建。

  - `~/Documents/knowledge_base`：目标路径，~ 是你的家目录（`/home/xiyu/`）。

* **作用**：确保有一个统一的地方存放知识库文档。



#### 2.将各种格式的文档复制到目录：

```bash
cp ~/Downloads/*.{txt,pdf,doc,docx,md,epub,pptx,ppt,wps,xls,xlsx} ~/Documents/knowledge_base/ 2>/dev/null
```

- **命令含义**：

  - `cp`：复制文件。
  - `*.{txt,pdf,...}`：使用花括号扩展匹配多种文件后缀。
  - `2>/dev/null`：忽略“无匹配文件”的错误（例如如果没有 `.wps` 文件）。

- **作用**：将所有指定格式的文档集中到 `~/Documents/knowledge_base/`。

- **注意**：如果文件在硬盘 2（Windows 11），先挂载：

```bash
sudo mount /dev/nvme0n1p2 /mnt/windows
cp /mnt/windows/Documents/*.{txt,pdf,doc,docx,md,epub,pptx,ppt,wps,xls,xlsx} ~/Documents/knowledge_base/
```

### 6.2 安装支持多种格式的依赖

是否需要在虚拟环境中执行？是
这些是 Python 库，必须在虚拟环境中安装。

#### 1.激活虚拟环境（如果未激活）：

```bash
source ~/knowledge_env/bin/activate
```

- **命令含义**：
  - `source`：运行脚本。
  - `~/knowledge_env/bin/activate`：激活虚拟环境的脚本。
- **作用**：进入虚拟环境，后续 `pip install` 只影响此环境。
- **结果**：提示符前显示 (`knowledge_env`)。

#### 2.安装扩展支持的库

```bash
pip install pypdf2 python-docx markdown epub python-pptx xlrd openpyxl
```

- **命令含义**：
  - `pip install`：安装 Python 包。
  - `pypdf2`：解析 `.pdf`。
  - `python-docx`：解析 `.doc` 和 `.docx`。
  - `markdown`：解析 `.md`。
  - `epub`：解析 `.epub`（需要额外处理）。
  - `python-pptx`：解析 `.pptx` 和 `.ppt`。
  - `xlrd`：解析 `.xls`。
  - `openpyxl`：解析 `.xlsx`。
- **作用**：让 LangChain 支持更多文件格式。
- **注意**：`.wps` 文件没有直接的 Python 解析库，建议先转为 `.doc` 或 `.pdf`（可用 WPS 软件转换）。

### 6.3 创建知识库脚本

是否需要在虚拟环境中执行？是
脚本依赖虚拟环境中的库。

#### 1.创建脚本目录并进入：

```bash
mkdir -p ~/Documents/PythonCode
cd ~/Documents/PythonCode
```

- **命令含义**：
  - `mkdir -p`：创建目录。
  - `cd`：切换目录。
- 作用：准备存放 Python 脚本的地方。

#### 2.创建脚本 `create_db.py`：

```bash
nano create_db.py
```

- 输入以下代码：

```python
import os
import nltk
# 添加 NLTK 数据路径
nltk.data.path.append(os.path.expanduser("~/nltk_data"))

from langchain_community.document_loaders import (
    DirectoryLoader, PyPDFLoader, Docx2txtLoader, TextLoader,
    UnstructuredEPubLoader, UnstructuredPowerPointLoader, UnstructuredExcelLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# 定义支持的文档类型和加载器
loader_map = {
    "*.txt": TextLoader,
    "*.md": TextLoader,
    "*.pdf": PyPDFLoader,
    "*.doc": Docx2txtLoader,
    "*.docx": Docx2txtLoader,
    "*.epub": UnstructuredEPubLoader,
    "*.pptx": UnstructuredPowerPointLoader,
    "*.ppt": UnstructuredPowerPointLoader,
    "*.xls": UnstructuredExcelLoader,
    "*.xlsx": UnstructuredExcelLoader
}

# 加载所有文档
documents = []
base_path = os.path.expanduser("~/Documents/knowledge_base")

# 遍历所有支持的文件类型
for pattern, loader_class in loader_map.items():
    loader = DirectoryLoader(base_path, glob=pattern, loader_cls=loader_class)
    try:
        loaded_docs = loader.load()
        print(f"成功加载了 {len(loaded_docs)} 个文档 for pattern {pattern}")
        if loaded_docs:  # 打印部分内容以验证
            print(f"示例内容: {loaded_docs[0].page_content[:100]}")
        documents.extend(loaded_docs)
    except Exception as e:
        print(f"加载 {pattern} 文件时出错: {e}")

# 检查是否加载到文档
if not documents:
    print("没有加载任何文档，退出程序。")
    exit(1)

# 分割文档
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
docs = text_splitter.split_documents(documents)
print(f"分割后文档数量: {len(docs)}")

# 创建嵌入
model_path = os.path.expanduser("~/models/all-MiniLM-L6-v2")
embeddings = HuggingFaceEmbeddings(model_name=model_path)

# 存储到 Chroma 数据库
db = Chroma.from_documents(docs, embeddings, persist_directory="~/knowledge_db")
db.persist()
print("数据库创建成功！")
```

- **代码解释**：
  - `loader_map`：映射文件类型到对应的加载器。
  - `DirectoryLoader`：加载指定路径的文档。
  - `try/except`：捕获加载错误（某些文件可能损坏或不受支持）。
  - `text_splitter`：将文档切成 1000 字符的小块，200 字符重叠。
  - `embeddings`：将文本转为向量。
  - `Chroma`：创建并保存向量数据库。
- 保存退出：按 `Ctrl + O`，回车，`Ctrl + X`。

#### 3.运行脚本：

```python
python create_db.py
```

- **命令含义**：执行脚本，生成知识库。
- **作用**：读取所有文档，创建数据库。
- **结果**：终端显示“知识库创建完成！”，并在 `~/knowledge_db` 生成数据库文件。

**注意**：如果缺少 unstructured 库（支持 `.epub`、`.pptx`、`.xls` 等），额外安装：

```bash
pip install unstructured
```

#### 补充0：安装 pandoc

需要在虚拟环境外进行安装，即安装至物理本机的系统环境中

```bash
sudo pacman -S pandoc
```



#### 补充1：all-MiniLM-L6-v2模型

##### **1.下载模型文件**：

- all-MiniLM-L6-v2 是 SentenceTransformers 的一个预训练模型。需要手动下载并放置到正确路径。
- 从以下网站下载：
  - [Hugging Face 镜像](https://hf-mirror.com/) 或 [ModelScope](https://www.modelscope.cn/models/)。
  - 搜索 sentence-transformers/all-MiniLM-L6-v2，下载模型文件。
- 下载后，解压文件到一个目录，例如 ~/models/all-MiniLM-L6-v2。

##### **2.确认目录结构**：

- 确保 ~/models/all-MiniLM-L6-v2 目录包含以下文件：

```text
config.json
pytorch_model.bin（或 model.safetensors）
tokenizer.json
```

##### **3.验证路径是否存在**：

```bash
ls ~/models/all-MiniLM-L6-v2
```

  - 如果目录为空或不存在，请将下载的模型文件移动到此路径。

##### **4.修改脚本中的路径**：

- 当前脚本使用 ~/models/all-MiniLM-L6-v2，但为了确保路径被正确解析，建议显式展开 ~。

- 编辑 `create_db.py`：


```bash
nano ~/Documents/PythonCode/create_db.py
```

将以下行：

```python
embeddings = SentenceTransformerEmbeddings(model_name="~/models/all-MiniLM-L6-v2")
```

替换为：

```python
import os
model_path = os.path.expanduser("~/models/all-MiniLM-L6-v2")
embeddings = SentenceTransformerEmbeddings(model_name=model_path)
```



#### **补充2**：NLTK 资源问题

> NLTK 资源问题
>
> ```bash
> $ > tree /home/xiyu/nltk_data/                                                                                                 
> /home/xiyu/nltk_data/
> ├── taggers
> │   ├── averaged_perceptron_tagger
> │   │   └── averaged_perceptron_tagger.pickle
> │   └── averaged_perceptron_tagger_eng
> │       ├── averaged_perceptron_tagger_eng.classes.json
> │       ├── averaged_perceptron_tagger_eng.tagdict.json
> │       └── averaged_perceptron_tagger_eng.weights.json
> └── tokenizers
>     ├── punkt
>     │   ├── czech.pickle
>     │   ├── danish.pickle
>     │   ├── dutch.pickle
>     │   ├── english.pickle
>     │   ├── estonian.pickle
>     │   ├── finnish.pickle
>     │   ├── french.pickle
>     │   ├── german.pickle
>     │   ├── greek.pickle
>     │   ├── italian.pickle
>     │   ├── malayalam.pickle
>     │   ├── norwegian.pickle
>     │   ├── polish.pickle
>     │   ├── portuguese.pickle
>     │   ├── PY3
>     │   │   ├── czech.pickle
>     │   │   ├── danish.pickle
>     │   │   ├── dutch.pickle
>     │   │   ├── english.pickle
>     │   │   ├── estonian.pickle
>     │   │   ├── finnish.pickle
>     │   │   ├── french.pickle
>     │   │   ├── german.pickle
>     │   │   ├── greek.pickle
>     │   │   ├── italian.pickle
>     │   │   ├── malayalam.pickle
>     │   │   ├── norwegian.pickle
>     │   │   ├── polish.pickle
>     │   │   ├── portuguese.pickle
>     │   │   ├── README
>     │   │   ├── russian.pickle
>     │   │   ├── slovene.pickle
>     │   │   ├── spanish.pickle
>     │   │   ├── swedish.pickle
>     │   │   └── turkish.pickle
>     │   ├── README
>     │   ├── russian.pickle
>     │   ├── slovene.pickle
>     │   ├── spanish.pickle
>     │   ├── swedish.pickle
>     │   └── turkish.pickle
>     └── punkt_tab
>         ├── czech
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── danish
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── dutch
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── english
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── estonian
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── finnish
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── french
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── german
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── greek
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── italian
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── malayalam
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── norwegian
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── polish
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── portuguese
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── README
>         ├── russian
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── slovene
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── spanish
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         ├── swedish
>         │   ├── abbrev_types.txt
>         │   ├── collocations.tab
>         │   ├── ortho_context.tab
>         │   └── sent_starters.txt
>         └── turkish
>             ├── abbrev_types.txt
>             ├── collocations.tab
>             ├── ortho_context.tab
>             └── sent_starters.txt
> 
> 27 directories, 121 files
> 
> ```
>
> 

##### **编号**1：下载`averaged_perceptron_tagger`和`punkt`

**a.下载**

- 从 NLTK 数据仓库手动下载：
  - averaged_perceptron_tagger：[下载链接](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/taggers/averaged_perceptron_tagger.zip)
  - punkt：[下载链接](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/tokenizers/punkt.zip)

**b.安装数据包**

- 创建本地 NLTK 数据目录并解压：

```bash
mkdir -p ~/nltk_data/taggers
unzip averaged_perceptron_tagger.zip -d ~/nltk_data/taggers
mkdir -p ~/nltk_data/tokenizers
unzip punkt.zip -d ~/nltk_data/tokenizers
```

**c.修改脚本指定路径**

- 在 create_db.py 开头添加以下代码：


```python
import nltk
nltk.data.path.append(os.path.expanduser("~/nltk_data"))
```

确保这些行位于导入其他模块之前。

**d.更新后的脚本开头**：

```python
import os
import nltk
nltk.data.path.append(os.path.expanduser("~/nltk_data"))

from langchain_community.document_loaders import (
    DirectoryLoader, PyPDFLoader, Docx2txtLoader, TextLoader,
    UnstructuredEPubLoader, UnstructuredPowerPointLoader, UnstructuredExcelLoader
)
```



~~~markdown
##### **编号**1.2：下载`punkt`

* Download the `punkt` Package:

Go to the NLTK data repository and download punkt.zip:[Direct Download Link](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/tokenizers/punkt.zip)

Save it to a convenient location (e.g., your Downloads folder).

* Extract and Place in NLTK Data Directory:

Create a directory at ~/nltk_data/tokenizers/ if it doesn’t already exist:

```bash
mkdir -p ~/nltk_data/tokenizers/
```

Unzip the downloaded file into this directory:

```bash
unzip ~/Downloads/punkt.zip -d ~/nltk_data/tokenizers/
```

After unzipping, you should see a punkt folder at ~/nltk_data/tokenizers/punkt/ containing files like english.pickle.
~~~



##### **编号**2：下载 `punkt_tab` 资源：

- 使用以下命令从 NLTK 数据仓库下载

```bash
wget https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/tokenizers/punkt_tab.zip -P ~/Downloads/
```

或手动访问 [NLTK 数据仓库](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/tokenizers/punkt_tab.zip) 下载。

* 解压并放置到正确目录：

创建目录（如果不存在）：

```bash
mkdir -p ~/nltk_data/tokenizers/punkt_tab
```

解压文件：

```bash
unzip ~/Downloads/punkt_tab.zip -d ~/nltk_data/tokenizers/punkt_tab
```

完成后，检查目录结构，确保存在 ~/nltk_data/tokenizers/punkt_tab/english/。



##### **编号**3：下载 `averaged_perceptron_tagger_eng`

* 使用浏览器访问以下链接下载文件：

```text
https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/taggers/averaged_perceptron_tagger_eng.zip
```

或者在有网络的设备上使用命令下载并传输到你的机器：

```bash
wget https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/taggers/averaged_perceptron_tagger_eng.zip -P ~/Downloads/
```

* 解压并放置到正确目录：

创建 NLTK 数据目录（如果不存在）：

```bash
mkdir -p ~/nltk_data/taggers
```

解压下载的文件：

```bash
unzip ~/Downloads/averaged_perceptron_tagger_eng.zip -d ~/nltk_data/taggers
```

完成后，检查目录，确保存在：

```text
~/nltk_data/taggers/averaged_perceptron_tagger_eng/
```



##### 验证资源小脚本

运行以下 Python 代码确认 NLTK 能找到资源：

```python
import nltk
import os
nltk.data.path.append(os.path.expanduser("~/nltk_data"))
print(nltk.data.find('taggers/averaged_perceptron_tagger_eng'))
```

如果成功，输出应类似：

```text
/home/xiyu/nltk_data/taggers/averaged_perceptron_tagger_eng
```

---



## 七、查询知识库

### 7.1 创建查询脚本

是否需要在虚拟环境中执行？是
依赖虚拟环境中的库。

为了与你的知识库互动，你需要创建一个 query.py 脚本，用于根据问题从数据库中检索相关文档，并使用 DeepSeek 模型生成答案。以下是具体操作：

#### 1.创建或更新 query.py  

- 打开终端，进入脚本目录：

```bash
#进入环境
source ~/knowledge_env/bin/activate

cd ~/Documents/PythonCode
```

使用文本编辑器创建或编辑 query.py：

```bash
nano query.py
```

输入以下代码：

```python
import os
from langchain_community.llms import LlamaCpp
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings

# 定义路径
model_path = os.path.expanduser("~/models/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf")
embeddings_model_path = os.path.expanduser("~/models/all-MiniLM-L6-v2")
persist_directory = os.path.expanduser("~/knowledge_db")

# 加载嵌入模型
embeddings = SentenceTransformerEmbeddings(model_name=embeddings_model_path)

# 加载 Chroma 数据库
db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)

# 初始化 LlamaCpp 模型
llm = LlamaCpp(
    model_path=model_path,
    n_gpu_layers=35,
    verbose=True
)

# 查询知识库的函数
def query_knowledge_base(query):
    docs = db.similarity_search(query)
    context = "\n".join([doc.page_content for doc in docs])
    prompt = f"根据以下上下文回答问题：{query}\n\n上下文：\n{context}"
    response = llm(prompt)
    print(response)

# 交互式循环
while True:
    query = input("请输入你的问题（输入 'exit' 退出）：")
    if query.lower() == 'exit':
        break
    query_knowledge_base(query)
```

​	保存并退出：按 Ctrl + O，回车，然后 Ctrl + X。

运行查询脚本  

```bash
python query.py
```

​	在终端输入问题，例如：“民法典关于财产权利的规定是什么？”按回车后，脚本会根据知识库中的内容生成回答。

```bash
#输入 exit 可退出交互模式。
> exit
```

注意事项  

- 确保 ~/models/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf 和 ~/models/all-MiniLM-L6-v2 路径正确，且文件完整。
- 如果模型加载失败，检查 GPU 内存是否足够（GTX 4060 8GB 显存通常够用）。



## 管理和优化的详细步骤

为了保持知识库的更新和性能，你可以按照以下方法进行管理和优化：

#### 更新知识库  

- 当你新增文件到 ~/Documents/knowledge_base 时（例如添加新的 .epub 或 .txt 文件），需要重新运行 create_db.py：


- ```bash
  python create_db.py
  ```

- 如果修改或删除文件，也需重新运行以更新数据库。

- 提示：每次更新后，旧的 ~/knowledge_db 会自动覆盖，确保备份重要数据。

#### 性能优化  

- 调整分割参数：如果你发现分割后的文档块（2095 个）过多，可调整 chunk_size 和 chunk_overlap：

  - 编辑 create_db.py，修改 RecursiveCharacterTextSplitter：

- ```python
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)
  ```

- 较大的 chunk_size 可减少块数，但可能影响搜索精度。

#### 优化 Chroma 搜索

Chroma 支持调整 k 参数（返回的相似文档数量），可在 query.py 中修改：

- ```python
  docs = db.similarity_search(query, k=5)  # 默认 k=4，可调整
  ```

#### 模型性能调整  

- 在 query.py 中，调整 LlamaCpp 的参数以优化回答：

  - n_gpu_layers：根据显存调整（35 适合 8GB 显存）。

  - temperature：控制生成文本的随机性，0.8 为默认，可尝试 0.5 以减少随机性。

  - 示例修改：

- ```python
  llm = LlamaCpp(
      model_path=model_path,
      n_gpu_layers=35,
      temperature=0.5,
      verbose=True
  )
  ```

注意：调整参数可能需要多次测试以找到最佳设置。

#### 常见问题与解决

以下是一些可能遇到的问题及其解决方案，帮助你顺利使用知识库：

| 问题                         | 可能原因                     | 解决方案                                                     |
| ---------------------------- | ---------------------------- | ------------------------------------------------------------ |
| 模型回答不准确               | 上下文不相关或查询不清晰     | 检查文档是否包含相关内容，优化查询语句，或调整 k 参数以返回更多上下文。 |
| Chroma 数据库未找到          | 路径错误或数据库未创建       | 确认 ~/knowledge_db 路径，运行 create_db.py 确保数据库创建。 |
| 嵌入模型加载失败             | 模型文件缺失或路径错误       | 检查 ~/models/all-MiniLM-L6-v2 是否包含 config.json 和 pytorch_model.bin。 |
| LlamaCpp 模型加载失败        | 模型文件损坏或不兼容         | 验证 ~/models/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf 文件完整性，重新下载。 |
| 性能慢，响应时间长           | 知识库过大或 GPU 资源不足    | 优化分割参数，减少文档块数，或升级硬件（如增加内存）。       |
| 网络问题（如果涉及在线资源） | 网络受限，无法下载模型或资源 | 确保所有文件本地化，下载模型到指定路径，避免在线依赖。       |

- 文档加载失败：如果新增文件类型加载失败，检查是否安装了对应依赖（如 unstructured 的完整依赖）。
- 警告处理：如之前提到的 Could not load translations for en-US，可忽略或安装 unstructured[all]：

```bash
pip install unstructured[all]
```

**性能监控**：

- 如果知识库规模增大，监控响应时间，必要时调整 chunk_size 或 k 参数。
- 使用 top 或 nvidia-smi 监控 GPU 和 CPU 使用率，确保资源充足。

**模型升级**：

- 如果 DeepSeek 模型性能不足，可考虑升级到更大参数的模型（如 13B），但需确认硬件支持（GTX 4060 8GB 显存可能需要量化）。

---

## Q1: 发现回答不完整，被意外截断

### 直接回答

- **关键点**：
  研究表明，优化本地知识库的回答可以从调整检索文档数量、改进提示词、调整模型参数和优化文档分割开始，这些方法似乎能提升回答的完整性。

#### 调整检索文档数量

为了让回答更完整，可以增加从知识库中检索的相似文档数量。例如，将 db.similarity_search(query, k=5) 设置为 k=5 或更高，获取更多上下文信息。

#### 改进提示词

优化提示词可以引导模型提供更详细的回答。尝试使用更具体的提示，如：“你是专家，请根据上下文详细回答问题，如果上下文不足，请说明。”

#### 调整模型参数

降低模型的温度（temperature）参数（如设置为 0.5）可以使回答更专注，减少随机性，提升准确性。

#### 优化文档分割

在 create_db.py 中调整文档分割参数，例如将 chunk_size 增大到 1500，chunk_overlap 设为 300，以确保上下文更连贯。

#### 意外细节

你可能没想到，调整这些参数需要多次测试才能找到最佳设置，这可能需要一些耐心和实验。

------

### 调查报告

以下是关于优化本地知识库回答的详细分析，涵盖所有相关步骤和建议，旨在帮助你提升回答的完整性和质量。

#### 背景与现状

你已经成功运行了 query.py，并能够通过交互式输入问题获取回答。然而，你发现模型的回答不完整，例如在回答“民法典中关于财产和权利的规定是什么？”时，回答被截断，需要手动输入“请继续”来获取更多内容。同样，在回答“如何系统学习《民法典》”时，回答也未完全展开。这些问题表明，当前设置可能在上下文提供、模型生成或提示设计上存在优化空间。

#### 数据质量与相关性

首先，确保知识库中的数据质量和相关性至关重要。你的知识库基于 ~/Documents/knowledge_base 中的文档（例如 .epub 文件），如果文档内容不全面或结构不清晰，模型可能无法生成完整的回答。建议定期检查和更新 knowledge_base 目录，确保包含所有相关信息。例如，如果文档是法律文本，确保涵盖了所有相关条款。

- 操作建议：新增或更新文件后，重新运行 create_db.py 更新数据库：

- ```bash
  python create_db.py
  ```

#### 查询优化：调整检索文档数量

模型的回答依赖于从 Chroma 数据库中检索的相似文档。如果检索的文档数量不足，上下文可能不够丰富，导致回答不完整。默认情况下，db.similarity_search(query) 通常返回 4 个最相似的文档。你可以增加 k 参数以获取更多上下文：

- 修改 query.py：
  - 将 docs = db.similarity_search(query) 改为 docs = db.similarity_search(query, k=5)，尝试 k=5 或更高。
  - 示例代码：

- - ```python
    def query_knowledge_base(query):
        docs = db.similarity_search(query, k=5)  # 增加到 5 个文档
        context = "\n".join([doc.page_content for doc in docs])
        prompt = f"你是专家，请根据以下上下文详细回答问题：{query}\n\n上下文：\n{context}\n请提供详细准确的答案，如果上下文不足，请说明。"
        response = llm(prompt)
        print(response)
    ```

- 效果：更多文档可能提供更丰富的上下文，但需注意避免引入过多无关信息。

#### 提示词工程：改进提示设计

提示词（prompt）是引导模型生成回答的关键。当前提示为：“根据以下上下文回答问题：{query}\n\n上下文：\n{context}”，较为简单。优化提示可以明确模型的角色和期望输出格式。例如：

- 改进提示：

- ```python
  prompt = f"""你是法律领域的专家，专门根据提供的上下文回答问题。
  
  问题：{query}
  
  上下文：
  {context}
  
  请提供详细且准确的答案，基于上下文。如果上下文不足以回答，请说明原因，并尝试推测可能的答案。
  
  回答："""
  ```

- 效果：更具体的提示可以帮助模型理解任务，生成更结构化的回答。

#### 模型参数调整：控制生成质量

DeepSeek-R1-Distill-Llama-8B-Q4_K_M 的生成参数（如温度 temperature）会影响回答的随机性和完整性。当前设置中，temperature 可能未明确指定，默认值可能较高，导致回答过于随机或不完整。建议降低温度以获得更专注的回答：

- 修改 query.py：
  - 在 LlamaCpp 初始化中添加 temperature 参数：

```python
llm = LlamaCpp(
    model_path=model_path,
    n_gpu_layers=35,
    temperature=0.5,  # 降低温度，减少随机性
    verbose=True
)
```

- 效果：温度较低（如 0.5）使模型更倾向于生成基于上下文的确定性回答，减少跳跃性内容。

#### 文档分割优化：调整上下文连贯性

在 create_db.py 中，文档分割参数（如 chunk_size 和 chunk_overlap）会影响上下文的连贯性。如果块太小，可能会丢失上下文，导致检索到的文档片段不完整。当前设置是 chunk_size=1000, chunk_overlap=200，你可以尝试调整：

- 修改 create_db.py：
  - 将分割参数调整为：

```python
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)
```

- 效果：更大的块大小和重叠部分可以确保上下文更连贯，检索时更可能包含完整信息。



## Q2：出现幻觉

关键点

- 研究表明，AI 模型的幻觉（hallucination）可以通过优化提示词、调整生成参数和提供上下文来减少。  
- 它似乎可能通过降低温度（temperature）参数（如 0.5）和增加检索文档数量来改善回答的准确性。  
- 意外的是，模型可能在法律问题上生成与上下文无关的内容，需验证输出与知识库的匹配度。

------

直接回答

AI 模型的幻觉可能通过以下方式减少：  

- 使用更具体的提示词，明确要求模型基于上下文回答。  
- 调整参数降低随机性，如设置温度为 0.5。  
- 确保知识库包含相关法律信息，并验证回答的准确性。

优化提示词

你可以修改提示词，例如：“你是法律专家，请根据以下上下文详细回答关于民法典的问题，仅基于上下文提供答案，避免额外猜测。”这能帮助模型专注于相关内容。  

调整生成参数

降低温度（如 0.5）可减少随机性，使回答更贴近上下文。增加检索文档数量（如从 4 个增至 5 个）也能提供更多背景信息。  

验证与管理

在问答后，建议对照知识库或官方法律文件（如 [中国民法典全文](https://www.npc.gov.cn/npc/c30834/202012/6e84e0a15d4a451a98c3c5ae5580b9b6.shtml)）验证答案，减少幻觉风险。  

------

------

调查报告

AI 模型的幻觉（hallucination）是指模型生成与输入或上下文无关、可能不准确或无意义的内容，尤其在处理法律问题如民法典时，这种现象可能导致误导性回答。你的日志显示，当询问“是否可以直接输出结果，暂时忽略你的思考过程”时，模型生成了一段关于孩子的故事，并回答了一个关于孩子是否能成为科学家的无关问题，这显然与民法典无关，表明存在幻觉。以下是详细分析和优化建议，旨在减少此类问题。

背景与现状

你已采用方法 3，使用 llama_cpp.Llama 直接管理模型资源，并在退出时显式释放（llm.close()），日志显示资源释放成功，但模型在回答时出现了幻觉。日志中，模型对“是否可以直接输出结果”这一问题未直接回应，而是生成了一段关于孩子的故事，并回答了“孩子是否能通过努力成为科学家”，这与预期（民法典相关问题）不符。  

从生成参数看，模型使用 temperature=0.5, top_k=50, top_p=0.95，这些参数旨在平衡创造性和准确性，但仍未完全避免幻觉。日志还显示上下文长度为 512，远低于模型训练的 131072，可能限制了长上下文的处理能力。  

幻觉原因分析

幻觉可能由以下因素引起：  

1. 提示词不足：当前提示词可能不够具体，模型未明确理解任务（如“直接输出结果”未明确限制输出范围）。  
2. 上下文不足：知识库可能未包含足够民法典相关信息，模型依赖训练数据生成无关内容。  
3. 生成参数设置：温度（0.5）可能仍导致一定随机性，top_k 和 top_p 设置可能引入无关词汇。  
4. 模型局限：DeepSeek-R1-Distill-Llama-8B-Q4_K_M 可能未针对法律领域充分优化，容易生成泛化内容。

优化策略

以下是减少幻觉的具体方法，结合你的使用场景（民法典相关问题）：  

1. 优化提示词设计

提示词是引导模型的关键，需明确任务和限制。建议修改为：  

- 示例提示：  

  python

- ```python
  prompt = f"""你是法律领域的专家，专门根据以下上下文回答关于民法典的问题。  
  问题：{query}  
  上下文：  
  {context}  
  请直接输出结果，基于上下文提供详细且准确的答案，避免额外猜测或无关内容。"""
  ```

- 效果：明确模型角色和输出要求，减少无关生成。  

- 注意：确保上下文包含民法典相关内容，必要时扩展知识库。

- 调整生成参数

生成参数影响模型输出随机性，建议进一步优化：  

- 降低温度：将 temperature 从 0.5 降低到 0.3，减少随机性：  

  python

```python
llm = Llama(model_path, n_gpu_layers=35, temperature=0.3, top_k=40, top_p=0.9, verbose=True)
```

调整 top_k 和 top_p：减少候选词范围，top_k=40, top_p=0.9 可聚焦于高概率输出。  

增加上下文长度：将 n_ctx 增加到 2048（如果硬件支持）：  

python

- ```python
  llm = Llama(model_path, n_gpu_layers=35, n_ctx=2048, temperature=0.3, verbose=True)
  ```

- 效果：更低的随机性可能减少幻觉，但需测试回答质量。

- 扩展知识库

确保 ~/Documents/knowledge_base 包含民法典相关文档（如 .epub 或 .pdf 文件），并定期更新：  

- 操作：将民法典全文（如 [中国民法典全文](https://www.npc.gov.cn/npc/c30834/202012/6e84e0a15d4a451a98c3c5ae5580b9b6.shtml)）加入知识库，运行：  

  bash

- ```bash
  python create_db.py
  ```

- 效果：更多相关上下文可提高回答准确性。

- 验证与后处理

模型输出可能仍含幻觉，建议验证：  

- 手动验证：对照知识库或官方法律文件（如 [中国民法典全文](https://www.npc.gov.cn/npc/c30834/202012/6e84e0a15d4a451a98c3c5ae5580b9b6.shtml)）检查答案。  
- 自动化验证：在脚本中添加简单规则，如检查输出是否包含“民法典”相关关键词，若无则提示“可能存在幻觉”。
- 模型选择与升级

如果幻觉频繁，考虑升级模型（如 13B 参数版本），但需确认硬件支持（GTX 4060 8GB 显存可能需量化）。  

退出异常处理

日志显示退出时出现 Exception ignored in: <function LlamaSampler.__del__ at 0x7cd46ddc0220> 和 TypeError: 'NoneType' object is not callable，这是资源清理问题，但不影响功能。建议接受异常，关注 llama_cpp 的更新（如 [Llama.cpp GitHub Issues](https://github.com/ggerganov/llama.cpp/issues)）。  

常见问题与解决

| 问题                   | 可能原因                     | 解决方案                                               |
| ---------------------- | ---------------------------- | ------------------------------------------------------ |
| 模型回答幻觉，内容无关 | 提示词不明确或上下文不足     | 优化提示词，扩展知识库，确保包含相关信息。             |
| 回答不完整             | 上下文长度不足或参数设置不优 | 增加 n_ctx（如 2048），调整 temperature 和 top_k。     |
| 性能慢，响应时间长     | 知识库过大或 GPU 资源不足    | 优化分割参数，减少文档块数，或升级硬件（如增加内存）。 |
| 退出时异常             | 资源清理问题，库 bug         | 接受异常，关注 llama_cpp 更新，必要时切换到新版本。    |

关键引用

- [中国民法典全文](https://www.npc.gov.cn/npc/c30834/202012/6e84e0a15d4a451a98c3c5ae5580b9b6.shtml)  
- [Llama.cpp GitHub Issues](https://github.com/ggerganov/llama.cpp/issues)  
- [LangChain 社区文档](https://python.langchain.com/docs/get_started/introduction)  
- [Chroma 官方文档](https://docs.trychroma.com/)  
- [Sentence Transformers 模型](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)

Key Citations

- [中国民法典全文页面](https://www.npc.gov.cn/npc/c30834/202012/6e84e0a15d4a451a98c3c5ae5580b9b6.shtml)  
- [Llama.cpp GitHub Issues页面](https://github.com/ggerganov/llama.cpp/issues)  
- [LangChain社区文档介绍页面](https://python.langchain.com/docs/get_started/introduction)  
- [Chroma官方文档页面](https://docs.trychroma.com/)  
- [Sentence Transformers all-MiniLM-L6-v2模型页面](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)

